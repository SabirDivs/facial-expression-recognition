{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2988b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa9300eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator(rescale= 1/255.0) #divides each pix value by 255\n",
    "validation = ImageDataGenerator(rescale = 1/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0522464d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 images belonging to 7 classes.\n",
      "Found 140 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train.flow_from_directory(\"C:\\\\Users\\\\Ihsan Ullah\\\\FERS\\\\Data\\\\facialexpression\\\\train\\\\\",\n",
    "                                            target_size= (48, 48),\n",
    "                                            batch_size = 7)\n",
    "                                            #class_mode = \"binary\")\n",
    "validation_dataset = validation.flow_from_directory(\"C:\\\\Users\\\\Ihsan Ullah\\\\FERS\\\\Data\\\\facialexpression\\\\validation\\\\\",\n",
    "                                            target_size= (48, 48),\n",
    "                                            batch_size = 7)\n",
    "                                            #class_mode = \"binary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b57dc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class_indices  #this class convets the class string into numbers, as neural network cannot take strings, so it converts all the classes names into a numerical value\n",
    "\n",
    "#validation_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b74aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 47, 47, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 46, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 45, 45, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 44, 44, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 43, 43, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 41, 41, 128)       32896     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_10  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 112,951\n",
      "Trainable params: 112,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#train_dataset.classes #Shows the lables assigned to the images of the classes as classes are given values 0, 1 and 2\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,(2,2), activation = \"relu\", input_shape = (48,48,1)),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=(2,2),\n",
    "        strides = 1,),\n",
    "    #\n",
    "    tf.keras.layers.Conv2D(32,(2,2), activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=(2,2),\n",
    "        strides = 1,),\n",
    "    #\n",
    "    tf.keras.layers.Conv2D(64,(2,2), activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size=(2,2),\n",
    "        strides = 1,),\n",
    "    ##\n",
    "    ##\n",
    "    tf.keras.layers.Conv2D(128,(2,2), activation = \"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    \n",
    "    ##\n",
    "    tf.keras.layers.Flatten(),\n",
    "    ##\n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    ##\n",
    "    tf.keras.layers.Dense(7, activation = \"softmax\")])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17fb399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.losses.CategoricalCrossentropy(\n",
    "# from_logits=false, label_smoothing=0,axis=-1, reduction=losses.utils.ReductionV2.AUTO, name = \"categorical)crossentropy\")\n",
    "\n",
    "model.compile(loss= tf.keras.losses.categorical_crossentropy,\n",
    "             optimizer = \"adam\",\n",
    "             metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "855f7584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 4s 651ms/step - loss: 1.9461 - accuracy: 0.1714 - val_loss: 1.9520 - val_accuracy: 0.1429\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 2s 551ms/step - loss: 1.9341 - accuracy: 0.2571 - val_loss: 1.9700 - val_accuracy: 0.1429\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 2s 562ms/step - loss: 1.9551 - accuracy: 0.1429 - val_loss: 1.9628 - val_accuracy: 0.1429\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 2s 510ms/step - loss: 1.9538 - accuracy: 0.1143 - val_loss: 1.9553 - val_accuracy: 0.1429\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 2s 516ms/step - loss: 1.9553 - accuracy: 0.1714 - val_loss: 1.9518 - val_accuracy: 0.1429\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 2s 501ms/step - loss: 1.9351 - accuracy: 0.2000 - val_loss: 1.9496 - val_accuracy: 0.1429\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 2s 551ms/step - loss: 1.9613 - accuracy: 0.0857 - val_loss: 1.9480 - val_accuracy: 0.1429\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 2s 503ms/step - loss: 1.9335 - accuracy: 0.2000 - val_loss: 1.9474 - val_accuracy: 0.1429\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 2s 506ms/step - loss: 1.9586 - accuracy: 0.0857 - val_loss: 1.9469 - val_accuracy: 0.1429\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 2s 522ms/step - loss: 1.9515 - accuracy: 0.2000 - val_loss: 1.9464 - val_accuracy: 0.1429\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 2s 513ms/step - loss: 1.9499 - accuracy: 0.1429 - val_loss: 1.9462 - val_accuracy: 0.1429\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 3s 573ms/step - loss: 1.9473 - accuracy: 0.1429 - val_loss: 1.9462 - val_accuracy: 0.1429\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 2s 536ms/step - loss: 1.9464 - accuracy: 0.1714 - val_loss: 1.9462 - val_accuracy: 0.1429\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 2s 556ms/step - loss: 1.9433 - accuracy: 0.2000 - val_loss: 1.9463 - val_accuracy: 0.1429\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 2s 517ms/step - loss: 1.9432 - accuracy: 0.0857 - val_loss: 1.9465 - val_accuracy: 0.1429\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 3s 560ms/step - loss: 1.9470 - accuracy: 0.1714 - val_loss: 1.9468 - val_accuracy: 0.1429\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 3s 575ms/step - loss: 1.9485 - accuracy: 0.1429 - val_loss: 1.9467 - val_accuracy: 0.1429\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 3s 561ms/step - loss: 1.9432 - accuracy: 0.0571 - val_loss: 1.9469 - val_accuracy: 0.1500\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 3s 616ms/step - loss: 1.9454 - accuracy: 0.0857 - val_loss: 1.9471 - val_accuracy: 0.1429\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 2s 557ms/step - loss: 1.9511 - accuracy: 0.1143 - val_loss: 1.9469 - val_accuracy: 0.1429\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 3s 625ms/step - loss: 1.9548 - accuracy: 0.0286 - val_loss: 1.9465 - val_accuracy: 0.1429\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 2s 564ms/step - loss: 1.9477 - accuracy: 0.1429 - val_loss: 1.9464 - val_accuracy: 0.1357\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 2s 547ms/step - loss: 1.9420 - accuracy: 0.1143 - val_loss: 1.9464 - val_accuracy: 0.1429\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 3s 597ms/step - loss: 1.9465 - accuracy: 0.1143 - val_loss: 1.9466 - val_accuracy: 0.1429\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 3s 577ms/step - loss: 1.9438 - accuracy: 0.1143 - val_loss: 1.9466 - val_accuracy: 0.1429\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 2s 547ms/step - loss: 1.9449 - accuracy: 0.0857 - val_loss: 1.9467 - val_accuracy: 0.1429\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 3s 579ms/step - loss: 1.9384 - accuracy: 0.1714 - val_loss: 1.9472 - val_accuracy: 0.1429\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 3s 569ms/step - loss: 1.9392 - accuracy: 0.1714 - val_loss: 1.9481 - val_accuracy: 0.1429\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 2s 532ms/step - loss: 1.9498 - accuracy: 0.0857 - val_loss: 1.9490 - val_accuracy: 0.1429\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 3s 583ms/step - loss: 1.9387 - accuracy: 0.2000 - val_loss: 1.9535 - val_accuracy: 0.1429\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit( train_dataset,\n",
    "                     steps_per_epoch = 5,\n",
    "                     epochs = 30,\n",
    "                     validation_data = validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f68201d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"test\")\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    cv2.imshow(\"test\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"test_image_{}.png\".format(img_counter) \n",
    "        path = 'C:\\\\Users\\\\Ihsan Ullah\\\\FERS\\\\Data\\\\facialexpression\\\\test\\\\'\n",
    "        cv2.imwrite(os.path.join(path , img_name), frame)\n",
    "#         shutil.move(\"C:\\\\Users\\\\Ihsan Ullah\\\\FRS\\\\FRS via Data Generator\\\\\" , \"C:\\\\Users\\\\Ihsan Ullah\\\\FRS\\\\FRS via Data Generator\\\\basedata\\\\Uni Class\\\\testing\\\\ihsan\\\\\")\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f471782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 200, 200, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200, 200, 3), dtype=tf.float32, name='conv2d_20_input'), name='conv2d_20_input', description=\"created by layer 'conv2d_20_input'\"), but it was called on an input with incompatible shape (None, 48, 48, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-4b6536563dcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#images are passed to model to predic which image belongs to which class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m# if the  image in testing dataset belongs to value class 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"angry\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "dir_path = \"C:\\\\Users\\\\Ihsan Ullah\\\\FERS\\\\Data\\\\facialexpression\\\\test\\\\\"\n",
    "for i in os.listdir(dir_path):\n",
    "    img= image.load_img(dir_path +\"\\\\\" + i, target_size = (48,48))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    X = image.img_to_array(img)#converts those images from testing dataset to array\n",
    "    X = np.expand_dims(X, axis = 0)#datagenerator is not provided here so for an aditional dimention here, expandDimention\n",
    "    images = np.vstack([X])#to vertical join multiple images\n",
    "    val = model.predict(images)#images are passed to model to predic which image belongs to which class\n",
    "    \n",
    "    if val == 0:# if the  image in testing dataset belongs to value class 0 \n",
    "        print(\"angry\")\n",
    "    elif val == 1:\n",
    "        print(\"disust\")\n",
    "    elif val == 2:\n",
    "        print(\"fear\")\n",
    "    elif val == 3:\n",
    "        print(\"happy\")\n",
    "    elif val == 4:\n",
    "        print(\"neutral\")\n",
    "    elif val == 5:\n",
    "        print(\"sad\")\n",
    "    else:\n",
    "        print(\"neutral\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60b0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
